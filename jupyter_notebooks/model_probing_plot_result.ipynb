{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "263658b8",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from quinine import QuinineArgumentParser\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e9b8ed4e",
   "metadata": {},
   "source": [
    "result_dir = '../results2/model_probe/'\n",
    "\n",
    "\n",
    "# this is for two L=20 models, both trained to full 500k, and probe_model trained for 100k\n",
    "result_name = {\n",
    "    'grad':{\n",
    "        'unloop': '0901115634_unloop_lr=0.001_target=grad_control=False_b5b6_diff_mlp_relu_L20_largeBatch',\n",
    "        'loop': '0831002516_loop_lr=0.001_target=grad_control=False_1ef6_diff_mlp_relu_L12_largeBatch'\n",
    "    },\n",
    "    'Wols':{\n",
    "        'unloop': '0901115634_unloop_lr=0.001_target=Wols_control=False_cbcf_diff_mlp_relu_L20_largeBatch',\n",
    "        'loop': '0831002516_loop_lr=0.001_target=Wols_control=False_bb76_diff_mlp_relu_L12_largeBatch'\n",
    "    }\n",
    "}\n",
    "\n",
    "control_result_name = {\n",
    "    'grad':{\n",
    "        'unloop': '0905121331_unloop_lr=0.001_target=grad_control=True_7cd7_diff_mlp_relu_L20_largeBatch',\n",
    "        'loop': '0907122856_loop_lr=0.001_target=grad_control=True_0dd6_diff_mlp_relu_L20_largeBatch'\n",
    "    },\n",
    "    'Wols':{\n",
    "        'unloop': '0904164926_unloop_lr=0.001_target=Wols_control=True_5c8b_diff_mlp_relu_L20_largeBatch',\n",
    "        'loop': '0904214357_loop_lr=0.001_target=Wols_control=True_f259_diff_mlp_relu_L20_largeBatch'\n",
    "    }\n",
    "}\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5a28a896",
   "metadata": {},
   "source": [
    "fig_hparam = {\n",
    "    'figsize': (8, 3),\n",
    "    'labelsize': 28,\n",
    "    'ticksize': 20,\n",
    "    'linewidth': 5,\n",
    "    'fontsize': 15,\n",
    "    'titlesize': 20,\n",
    "    'markersize': 15\n",
    "}\n",
    "\n",
    "# font specification\n",
    "fontdict = {'family': 'serif',\n",
    "         'size': fig_hparam['fontsize'],\n",
    "         }\n",
    "\n",
    "unloop_color = '#3b4cc0'\n",
    "loop_color = '#b40426'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeafa3e",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "11bec6fa",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for target_mode in result_name.keys():\n",
    "    fig, ax = plt.subplots(1, figsize=fig_hparam['figsize'])\n",
    "    for loop_mode in result_name[target_mode]:\n",
    "        print(target_mode, loop_mode)\n",
    "        if loop_mode == 'unloop':\n",
    "            offset = -0.2\n",
    "            n_points = 21\n",
    "            color = unloop_color\n",
    "            label = 'Transformer'\n",
    "        else:\n",
    "            offset = 0.2\n",
    "            n_points = 21\n",
    "            color = loop_color\n",
    "            label = 'Looped Transformer'\n",
    "        filename = result_name[target_mode][loop_mode]\n",
    "        tmp = torch.load(result_dir + filename + '/state_dict.pt', map_location=torch.device('cpu'))\n",
    "        print(tmp['p_loss'])\n",
    "\n",
    "        plt.rc('font', family='serif')\n",
    "        ax.bar(np.arange(n_points) + offset, tmp['p_loss'] / 20, 0.4, label=label, color=color)\n",
    "        \n",
    "        # load the control=True results\n",
    "        filename = control_result_name[target_mode][loop_mode]\n",
    "        tmp = torch.load(result_dir + filename + '/state_dict.pt', map_location=torch.device('cpu'))\n",
    "#         print(tmp['p_loss'])\n",
    "        ax.axhline(tmp['p_loss'].min() / 20, color=color, ls='--')\n",
    "\n",
    "    plt.ylim([0, 0.05])\n",
    "    plt.xlabel('Layer / Loop Iteration', fontsize=20)\n",
    "    plt.ylabel('MSE', fontsize=20)\n",
    "    plt.tick_params(axis='both', labelsize=fig_hparam['ticksize'])\n",
    "    if target_mode == 'grad':\n",
    "        plt.title(r\"$X^TY$ probe error\", fontsize=20)\n",
    "    else:\n",
    "        plt.title(r\"$\\mathbf{w}_{OLS}$ probe error\", fontsize=20)\n",
    "    plt.savefig(\"result_folder/Figures/model_probe_{}.pdf\".format(target_mode), format='pdf', dpi=600, bbox_inches='tight')\n",
    "    \n",
    "#     plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3ba71d1c",
   "metadata": {},
   "source": [
    "# Only plot the legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0db6feed",
   "metadata": {},
   "source": [
    "for target_mode in result_name.keys():\n",
    "    fig, ax = plt.subplots(1, figsize=fig_hparam['figsize'])\n",
    "    for loop_mode in result_name[target_mode]:\n",
    "        print(target_mode, loop_mode)\n",
    "        if loop_mode == 'unloop':\n",
    "            offset = -0.2\n",
    "            n_points = 21\n",
    "            color = unloop_color\n",
    "            label = 'Transformer'\n",
    "        else:\n",
    "            offset = 0.2\n",
    "            n_points = 21\n",
    "            color = loop_color\n",
    "            label = 'Looped Transformer'\n",
    "        filename = result_name[target_mode][loop_mode]\n",
    "        tmp = torch.load(result_dir + filename + '/state_dict.pt', map_location=torch.device('cpu'))\n",
    "        print(tmp['p_loss'])\n",
    "\n",
    "        plt.rc('font', family='serif')\n",
    "        ax.bar(np.arange(n_points) + offset, tmp['p_loss'] / 20, 0.4, label=label, color=color)\n",
    "        \n",
    "        # load the control=True results\n",
    "        filename = control_result_name[target_mode][loop_mode]\n",
    "        tmp = torch.load(result_dir + filename + '/state_dict.pt', map_location=torch.device('cpu'))\n",
    "#         print(tmp['p_loss'])\n",
    "        ax.axhline(tmp['p_loss'].min() / 20, color=color, ls='--')\n",
    "    \n",
    "\n",
    "    plt.ylim([0, 0.05])\n",
    "    plt.xlabel('layer')\n",
    "    plt.ylabel('squared error')\n",
    "    legend = plt.legend(ncol=2, loc='center left', bbox_to_anchor=(1, 0.5), fontsize=30, )\n",
    "    plt.savefig(\"result_folder/Figures/model_probe_legend.pdf\", format='pdf', dpi=600, bbox_inches='tight')\n",
    "    \n",
    "#     plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870738bc",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd056a",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
