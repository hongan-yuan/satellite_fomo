{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f26ad3-526e-4651-a715-6f1d9dd550ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from quinine import QuinineArgumentParser\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from nano_gpt import GPT2Model, GPT2Config\n",
    "from models import TransformerModelLooped\n",
    "from utils import aggregate_metrics, get_model, eval_unlooped_model, eval_looped_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a56a2b65-b484-4d88-a4af-39c82a5e6fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "fig_hparam = {\n",
    "    'figsize': (8, 5),\n",
    "    'labelsize': 28,\n",
    "    'ticksize': 20,\n",
    "    'linewidth': 5,\n",
    "    'fontsize': 15,\n",
    "    'titlesize': 20,\n",
    "    'markersize': 15\n",
    "}\n",
    "\n",
    "# font specification\n",
    "fontdict = {\n",
    "    'family': 'serif',\n",
    "    'size': fig_hparam['fontsize'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0356e462-7f1c-4c07-9bb4-489999ae1f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, batch_size, n_points, n_dims, n_dims_truncated, device, depth=4):\n",
    "        \"\"\"\n",
    "        batch_size: 1280\n",
    "        n_points: 101\n",
    "        n_dims: 20\n",
    "        n_dims_truncated: 20\n",
    "        device: torch.device('cuda:0')\n",
    "        depth: 4\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.n_points = n_points\n",
    "        self.n_dims = n_dims\n",
    "        self.n_dims_truncated = n_dims_truncated\n",
    "        self.depth = depth\n",
    "\n",
    "        # We represent the tree using an array (tensor). Root node is at index 0, its 2 children at index 1 and 2...\n",
    "        # dt_tensor stores the coordinate used at each node of the decision tree.\n",
    "        # Only indices corresponding to non-leaf nodes are relevant\n",
    "        self.decisionTree_tensor = torch.randint(\n",
    "            low=0,\n",
    "            high=n_dims,\n",
    "            size=(batch_size, 2 ** (depth + 1) - 1)  # size=(1280, 31)\n",
    "        )\n",
    "\n",
    "        # Target value at the leaf nodes.\n",
    "        # Only indices corresponding to leaf nodes are relevant.\n",
    "        self.target_tensor = torch.randn(self.decisionTree_tensor.shape)\n",
    "\n",
    "        self.xs = torch.randn(batch_size, n_points, n_dims).to(device)  # [B, n, d]\n",
    "        self.ys = self.evaluate(self.xs)\n",
    "\n",
    "    def evaluate(self, xs_b):\n",
    "        dt_tensor = self.decisionTree_tensor.to(xs_b.device)\n",
    "        target_tensor = self.target_tensor.to(xs_b.device)\n",
    "\n",
    "        ys_b = torch.zeros(\n",
    "            xs_b.shape[0],  # 1280\n",
    "            xs_b.shape[1],  # 101\n",
    "            device=xs_b.device\n",
    "        )\n",
    "\n",
    "        for i in range(xs_b.shape[0]):\n",
    "            xs_bool = xs_b[i] > 0\n",
    "            # If a single decision tree present, use it for all the xs in the batch.\n",
    "            if self.batch_size == 1:\n",
    "                dt = dt_tensor[0]\n",
    "                target = target_tensor[0]\n",
    "            else:\n",
    "                dt = dt_tensor[i]\n",
    "                target = target_tensor[i]\n",
    "            cur_nodes = torch.zeros(xs_b.shape[1], device=xs_b.device).long()\n",
    "            for j in range(self.depth):\n",
    "                cur_coords = dt[cur_nodes]\n",
    "                cur_decisions = xs_bool[torch.arange(xs_bool.shape[0]), cur_coords]\n",
    "                cur_nodes = 2 * cur_nodes + 1 + cur_decisions\n",
    "\n",
    "            ys_b[i] = target[cur_nodes]\n",
    "\n",
    "        return ys_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3949568-bbc2-4bba-8669-374c92e39b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_size = 1280\n",
    "batch_size = 32\n",
    "n_points = 101\n",
    "n_dims_truncated = 20\n",
    "n_dims = 20\n",
    "result_dir = '../results2/decision_tree_loop'\n",
    "run_id = '0926061635-DT_loop_L1_endsb70_T15-0602'\n",
    "\n",
    "real_task = DecisionTree(\n",
    "    batch_size=sample_size,  # 1280\n",
    "    n_points=n_points,  # 101\n",
    "    n_dims=n_dims,  # 20\n",
    "    n_dims_truncated=n_dims_truncated,  # 20\n",
    "    device=torch.device('cuda:0')\n",
    ")\n",
    "\n",
    "xs, ys = real_task.xs, real_task.ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb212ee6-91c4-43ef-b4f3-6fa641349f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 0.79M\n",
      ">>>>>>>>>>> ... model_path: ../results2/decision_tree_loop\\0926061635-DT_loop_L1_endsb70_T15-0602\\state.pt\n",
      ">>>>>>>>>>> ... model:TransformerModelLooped(\n",
      "  (_read_in): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (_backbone): GPT2Model(\n",
      "    (transformer): ModuleDict(\n",
      "      (wpe): Embedding(203, 256)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0): Block(\n",
      "          (ln_1): LayerNorm()\n",
      "          (attn): CausalSelfAttention(\n",
      "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
      "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm()\n",
      "          (mlp): MLP(\n",
      "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm()\n",
      "    )\n",
      "  )\n",
      "  (_read_out): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_dims=20\n",
    "n_positions=101\n",
    "n_embd=256\n",
    "n_layer=1\n",
    "n_head = 8\n",
    "model = TransformerModelLooped(n_dims, n_positions, n_embd, n_layer, n_head)\n",
    "step = -1\n",
    "model = get_model(model, result_dir, run_id, step)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84da28eb-904b-4578-9ade-9cc1b14562de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xs_train = xs[0: 32]\n",
    "ys_train = ys[0: 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "297d6966-2d2f-4516-8b16-abfdc1b8a719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs_train.shape ... torch.Size([32, 101, 20])\n",
      "ys_train.shape ... torch.Size([32, 101])\n"
     ]
    }
   ],
   "source": [
    "print(f\"xs_train.shape ... {xs_train.shape}\")\n",
    "print(f\"ys_train.shape ... {ys_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fab5f16-f926-44cf-bc98-229fb3d5a8cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> ys_b_wide.shape ...is... torch.Size([32, 101, 20])\n",
      ">>>>> embeds.shape ...is... torch.Size([32, 202, 256])\n",
      ">>>>> pred_list[0].shape ...is... torch.Size([32, 101])\n",
      "y_pred_list.length ... is 200\n",
      "y_pred_list[0].shape ... is torch.Size([32, 101])\n",
      "y_pred_list[1].shape ... is torch.Size([32, 101])\n",
      "y_pred_list[-1].shape ... is torch.Size([32, 101])\n",
      "y_pred_total.length ... is 1280\n",
      ">>>>>> ... tmp_list.length ..is.. 200\n",
      ">>>>>> ... tmp_list[0].shape ..is.. torch.Size([32, 1])\n",
      ">>>>>> ... tmp_array.shape ..is.. torch.Size([32, 200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0797e-01, 7.1042e-03, 7.7393e-02,  ..., 2.5754e-04, 2.7099e-04,\n",
       "         2.6570e-04],\n",
       "        [3.7938e-01, 8.1737e-02, 3.3259e-02,  ..., 1.4428e-04, 1.7628e-04,\n",
       "         1.2832e-04],\n",
       "        [4.3563e+00, 1.6584e+00, 1.7262e+00,  ..., 5.3808e-05, 6.7921e-05,\n",
       "         5.3269e-05],\n",
       "        ...,\n",
       "        [7.5778e-01, 7.5778e-01, 7.5778e-01,  ..., 7.5778e-01, 7.5778e-01,\n",
       "         7.5778e-01],\n",
       "        [1.7536e-01, 1.7536e-01, 1.7536e-01,  ..., 1.7536e-01, 1.7536e-01,\n",
       "         1.7536e-01],\n",
       "        [1.5212e-01, 1.5212e-01, 1.5212e-01,  ..., 1.5212e-01, 1.5212e-01,\n",
       "         1.5212e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred_total = torch.zeros(1280, 101)  # [N, n]\n",
    "    y_pred_last = torch.zeros(1280, 200)  # [N, T]  T refers to the number of loops.\n",
    "    for batch_idx in range(sample_size // batch_size):\n",
    "        xs_train = xs[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
    "        ys_train = ys[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
    "        # Record the results of each loop iteration.\n",
    "        y_pred_list = model(xs_train, ys_train, 0, 200)  # list of [B, n], length T\n",
    "        \n",
    "        #  get the last y_value from the list whose length equals to 101\n",
    "        last_y_pred_list = [y_pred[:, [-1]] for y_pred in y_pred_list]   # list of [B, 1], length T,\n",
    "        # Record the last y_value for each looped iteration can contact\n",
    "        last_y_pred_array = torch.cat(last_y_pred_list, dim=1)  # [B, T]  \n",
    "        print(f\">>>>>> ... last_y_pred_array.shape ..is.. {last_y_pred_array.shape}\")\n",
    "        \n",
    "        y_pred_last[batch_idx * batch_size: (batch_idx + 1) * batch_size] = last_y_pred_array\n",
    "\n",
    "        # The computation result of last iteration for (xs & ys) ==> the predicted ys' ==>  y_pred_list[-1].detach()\n",
    "        # y_pred_list[-1].shape ... is torch.Size([32, 101])\n",
    "        y_pred_total[batch_idx * batch_size: (batch_idx + 1) * batch_size] = y_pred_list[-1].detach()\n",
    "        print(f\"y_pred_total.length ... is {len(y_pred_total)}\")\n",
    "        \n",
    "    total_err = (y_pred_total - ys.cpu()).square()  # [n,]\n",
    "    loop_iter_err = (y_pred_last - ys.cpu()[:, [-1]]).square()  # [N, T] - [N, 1]\n",
    "loop_iter_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "396e0f24-29d2-4503-9408-64ad29b15b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1280, 200])\n",
      "torch.Size([1280, 101])\n",
      "torch.Size([1280, 101])\n"
     ]
    }
   ],
   "source": [
    "print(loop_err.shape)\n",
    "print(err.shape)\n",
    "print(y_pred_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9815537b-1f92-4c7a-b411-0a2854b5fc25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.0299e-01, 3.5761e-02, 1.4700e-02, 2.1203e-01, 1.4089e+00, 1.9964e-01,\n",
       "        8.7453e-03, 8.8008e-01, 2.2243e-02, 3.0616e-01, 3.2261e-01, 2.4495e-01,\n",
       "        9.1069e-01, 6.2506e-03, 6.3337e-02, 4.5949e-02, 1.7566e-01, 5.4632e-03,\n",
       "        1.6040e-02, 1.3257e-01, 3.9896e-01, 2.4753e-01, 6.1265e-03, 1.2358e-02,\n",
       "        2.4915e-03, 7.0874e-02, 1.8792e-05, 6.1906e-02, 5.0660e-01, 1.6655e-04,\n",
       "        1.7580e-01, 4.0564e-03, 3.6996e-01, 6.0228e-04, 5.5347e-03, 3.5812e-07,\n",
       "        1.5724e-03, 1.1645e-03, 5.4598e-02, 1.1050e-07, 1.0961e-06, 2.5093e-05,\n",
       "        5.5684e-06, 2.3651e-01, 9.1661e-02, 7.1861e-05, 4.8799e-03, 6.7404e-04,\n",
       "        5.0427e-02, 1.4975e-06, 1.9749e-05, 2.3235e-01, 1.3116e-03, 1.7315e-05,\n",
       "        1.5604e-03, 7.7848e-05, 1.1005e-02, 1.5388e-04, 8.5312e-03, 5.0744e-04,\n",
       "        3.4819e-05, 1.3055e-07, 5.6042e-06, 6.8602e-05, 6.7886e-06, 4.1041e-03,\n",
       "        1.5254e-04, 1.0620e-04, 2.0148e-05, 1.9869e-01, 1.1185e+00, 7.8692e-05,\n",
       "        9.9935e-05, 1.8576e-04, 1.4643e-04, 6.3445e-06, 4.1343e-07, 3.0588e-01,\n",
       "        1.6866e-04, 7.6630e-05, 2.8259e-06, 2.5395e-01, 7.6822e-04, 6.5563e-04,\n",
       "        2.2477e-04, 1.3117e-08, 2.8885e-04, 1.2449e-04, 1.2921e-04, 7.4214e-06,\n",
       "        1.4149e-03, 9.0580e-03, 1.7255e-04, 1.2560e-05, 5.0488e-04, 1.0285e-02,\n",
       "        1.3343e-04, 3.1709e-04, 2.5629e-04, 6.5133e-05, 2.6570e-04])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860deca-9d73-4771-8103-0df302b6018c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
