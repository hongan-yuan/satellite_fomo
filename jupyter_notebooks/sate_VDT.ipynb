{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf961dd-43ad-4ad4-b33f-c5cd8b748f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from quinine import QuinineArgumentParser\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from nano_gpt import GPT2Model, GPT2Config\n",
    "from models import TransformerModel\n",
    "from utils import aggregate_metrics, get_model, eval_unlooped_model, eval_looped_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af69a6a-7249-407f-afc5-9e2aca2f590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "result_dir = '../results2/decision_tree_baseline'\n",
    "run_id = '0926062109-DT_baseline-2504'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e360d0b-d940-4f4d-af16-733b6d9166fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 1280\n",
    "BATCH_SIZE = 32\n",
    "CONTEXT_SIZE = 101\n",
    "INPUT_DIMS = 20\n",
    "N_DIMS_TRUNCATED = 20\n",
    "EMBEDDING_DIM = 256\n",
    "N_HEAD = 8\n",
    "N_LAYER = 12\n",
    "N_LAYER_LOOP = 1\n",
    "# ------------------------------------------------\n",
    "LOOP_ITER_NUM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d16e1e8-3c31-4edf-b764-351bc6601ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self,\n",
    "                 batch_size, n_points, n_dims, n_dims_truncated, device, depth=4):\n",
    "        \"\"\"\n",
    "        batch_size: 1280\n",
    "        n_points: 101\n",
    "        n_dims: 20\n",
    "        n_dims_truncated: 20\n",
    "        device: torch.device('cuda:0')\n",
    "        depth: 4\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.n_points = n_points\n",
    "        self.n_dims = n_dims\n",
    "        self.n_dims_truncated = n_dims_truncated\n",
    "        self.depth = depth\n",
    "\n",
    "        # We represent the tree using an array (tensor). Root node is at index 0, its 2 children at index 1 and 2...\n",
    "        # dt_tensor stores the coordinate used at each node of the decision tree.\n",
    "        # Only indices corresponding to non-leaf nodes are relevant\n",
    "        self.decisionTree_tensor = torch.randint(\n",
    "            low=0,\n",
    "            high=n_dims,\n",
    "            size=(batch_size, 2 ** (depth + 1) - 1)  # size=(1280, 31)\n",
    "        )\n",
    "\n",
    "        # Target value at the leaf nodes.\n",
    "        # Only indices corresponding to leaf nodes are relevant.\n",
    "        self.target_tensor = torch.randn(self.decisionTree_tensor.shape)\n",
    "\n",
    "        self.xs = torch.randn(batch_size, n_points, n_dims).to(device)  # [B, n, d]\n",
    "        self.ys = self.evaluate(self.xs)\n",
    "\n",
    "    def evaluate(self, xs_b):\n",
    "        dt_tensor = self.decisionTree_tensor.to(xs_b.device)\n",
    "        target_tensor = self.target_tensor.to(xs_b.device)\n",
    "\n",
    "        ys_b = torch.zeros(\n",
    "            xs_b.shape[0],  # 1280\n",
    "            xs_b.shape[1],  # 101\n",
    "            device=xs_b.device\n",
    "        )\n",
    "\n",
    "        for i in range(xs_b.shape[0]):\n",
    "            xs_bool = xs_b[i] > 0\n",
    "            # If a single decision tree present, use it for all the xs in the batch.\n",
    "            if self.batch_size == 1:\n",
    "                dt = dt_tensor[0]\n",
    "                target = target_tensor[0]\n",
    "            else:\n",
    "                dt = dt_tensor[i]\n",
    "                target = target_tensor[i]\n",
    "            cur_nodes = torch.zeros(xs_b.shape[1], device=xs_b.device).long()\n",
    "            for j in range(self.depth):\n",
    "                cur_coords = dt[cur_nodes]\n",
    "                cur_decisions = xs_bool[torch.arange(xs_bool.shape[0]), cur_coords]\n",
    "                cur_nodes = 2 * cur_nodes + 1 + cur_decisions\n",
    "\n",
    "            ys_b[i] = target[cur_nodes]\n",
    "\n",
    "        return ys_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e37a1f6-5738-4251-99c5-3dced218ca4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load test data\n",
    "decision_tree_task = DecisionTree(\n",
    "    batch_size=SAMPLE_SIZE,  # 1280\n",
    "    n_points=CONTEXT_SIZE,  # 101\n",
    "    n_dims=INPUT_DIMS,  # 20\n",
    "    n_dims_truncated=N_DIMS_TRUNCATED,  # 20\n",
    "    device=torch.device('cuda:0')\n",
    ")\n",
    "xs, ys = decision_tree_task.xs, decision_tree_task.ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5d42bc-95aa-48bf-add3-7bea55a63c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1280, 101, 20])\n",
      "torch.Size([1280, 101])\n"
     ]
    }
   ],
   "source": [
    "print(xs.shape)\n",
    "print(ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfe41356-502b-4926-936c-e7a4ed2f29e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 9.48M\n",
      ">>>>>>>>>>> ... model_path: ../results2/decision_tree_baseline\\0926062109-DT_baseline-2504\\state.pt\n",
      ">>>>>>>>>>> ... model:TransformerModel(\n",
      "  (_read_in): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (_backbone): GPT2Model(\n",
      "    (transformer): ModuleDict(\n",
      "      (wpe): Embedding(203, 256)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0): Block(\n",
      "          (ln_1): LayerNorm()\n",
      "          (attn): CausalSelfAttention(\n",
      "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
      "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm()\n",
      "          (mlp): MLP(\n",
      "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Block(\n",
      "          (ln_1): LayerNorm()\n",
      "          (attn): CausalSelfAttention(\n",
      "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
      "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm()\n",
      "          (mlp): MLP(\n",
      "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): Block(\n",
      "          (ln_1): LayerNorm()\n",
      "          (attn): CausalSelfAttention(\n",
      "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
      "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm()\n",
      "          (mlp): MLP(\n",
      "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): Block(\n",
      "          (ln_1): LayerNorm()\n",
      "          (attn): CausalSelfAttention(\n",
      "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
      "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm()\n",
      "          (mlp): MLP(\n",
      "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): Block(\n",
      "          (ln_1): LayerNorm()\n",
      "          (attn): CausalSelfAttention(\n",
      "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
      "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm()\n",
      "          (mlp): MLP(\n",
      "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): Block(\n",
      "          (ln_1): LayerNorm()\n",
      "          (attn): CausalSelfAttention(\n",
      "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
      "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm()\n",
      "          (mlp): MLP(\n",
      "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): Block(\n",
      "          (ln_1): LayerNorm()\n",
      "          (attn): CausalSelfAttention(\n",
      "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
      "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm()\n",
      "          (mlp): MLP(\n",
      "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): Block(\n",
      "          (ln_1): LayerNorm()\n",
      "          (attn): CausalSelfAttention(\n",
      "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
      "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm()\n",
      "          (mlp): MLP(\n",
      "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): Block(\n",
      "          (ln_1): LayerNorm()\n",
      "          (attn): CausalSelfAttention(\n",
      "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
      "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm()\n",
      "          (mlp): MLP(\n",
      "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): Block(\n",
      "          (ln_1): LayerNorm()\n",
      "          (attn): CausalSelfAttention(\n",
      "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
      "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm()\n",
      "          (mlp): MLP(\n",
      "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): Block(\n",
      "          (ln_1): LayerNorm()\n",
      "          (attn): CausalSelfAttention(\n",
      "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
      "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm()\n",
      "          (mlp): MLP(\n",
      "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): Block(\n",
      "          (ln_1): LayerNorm()\n",
      "          (attn): CausalSelfAttention(\n",
      "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
      "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm()\n",
      "          (mlp): MLP(\n",
      "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm()\n",
      "    )\n",
      "  )\n",
      "  (_read_out): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (_read_in): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (_backbone): GPT2Model(\n",
       "    (transformer): ModuleDict(\n",
       "      (wpe): Embedding(203, 256)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (_read_out): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Looped Model\n",
    "model = TransformerModel(\n",
    "    n_dims=INPUT_DIMS,\n",
    "    n_positions=CONTEXT_SIZE,\n",
    "    n_embd=EMBEDDING_DIM,\n",
    "    n_layer=N_LAYER,\n",
    "    n_head=N_HEAD\n",
    ")\n",
    "\n",
    "step = -1\n",
    "\n",
    "model = get_model(model, result_dir, run_id, step)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3774a1c2-68f6-4add-b1c1-30c1d1fefb83",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bs = 256\n",
    "xs_train = xs[0: bs]\n",
    "ys_train = ys[0: bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7942ecc0-3d93-40a9-b653-fbf0385bdad3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 101])\n",
      "torch.Size([256, 101])\n",
      "torch.Size([256, 101])\n",
      "torch.Size([256, 101])\n",
      "torch.Size([256, 101])\n",
      "torch.Size([256, 101])\n",
      "torch.Size([256, 101])\n",
      "torch.Size([256, 101])\n",
      "10.7 ms ± 953 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "with torch.no_grad():\n",
    "    result_y = model(xs_train, ys_train)  # [1280, 101]\n",
    "print(result_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb8eda7-abf6-4cfe-9d5e-3832c7e4a9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
